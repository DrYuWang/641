\documentclass[12pt]{amsart}

\usepackage{mathrsfs, fullpage, amsmath, amssymb, graphicx, xcolor, tikz}

\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}

\input{../../macros.tex}

\begin{document}
\title{Regularization}
\maketitle

\section{Multiple linear regression}
Convention: We view $\RR^k$ as a subset of $\RR^{k+1}$ via the following identification
\begin{equation}\label{E:identification}
    v\in\RR^k\quad\longleftrightarrow\quad (1, v)\in\RR^{k+1}.
\end{equation}
    
$p-1$ predictor variables:
\[
(x_1, y_1),\ldots,(x_n, y_n)\in \RR^{1\times (p-1)}\times \RR
\]

Viewing $x_i$ as an element of $\RR^{1\times p}$ via~\eqref{E:identification}, define:
\[
    x := \begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}\in\RR^{n\times p},
    \quad y := \begin{bmatrix}
        y_1\\\vdots\\y_n
    \end{bmatrix}\in\RR^{n\times 1}
\]
For $\beta\in\RR^{p\times 1}$, consider the equation:
\[
    x\beta = y
\]
Equivalently:
\[
    \beta_0 + \beta_1x_{i,1} + \cdots \beta_{p-1}x_{i,p-1} = y_i,\quad i=1,\ldots,n.
\]

\textbf{Recall:}
The \emph{column space of $x$} is the subspace $C(x)$ of $\RR^{n\times 1}$
characterized by any of the following equivalent conditions:
\begin{itemize}\setlength\itemsep{0.5em}
    \item $C(x)$ is the set of all linear combinations of the columns of $x$
    \item $C(x)=\{x\beta : \beta\in\RR^{p\times 1}\}$
    \item $C(x)=\{y\in\RR^{n\times 1} : \text{$x\beta=y$ has a solution}\}$
\end{itemize}
$C(x)$ is also called the \emph{image of $x$}.

Let $\hat y\in\RR^{n\times 1}$ be the vector characterized by any of the
equivalent conditions:
\begin{itemize}\setlength\itemsep{0.5em}
    \item $\displaystyle{\hat y = \argmin\limits_{z\in C(x)}\|z - y\|}$
    \item $\hat y$ is the vector in the column space of $x$ closest to $y$.
    \item $\hat y$ is the orthogonal projection of $y$ onto the column space of $x$.
\end{itemize}
In partricular, $x\beta = \hat y$ has a solution.

\section{The case $\rank(x)=p$}
Suppose $\rank(x)=p$. Then $\beta\mapsto x\beta$ maps $\RR^{p\times 1}$
bijectively onto $C(x)$ and, therefore, there is a unique vector $\hat\beta\in\RR^{p\times 1}$
--- the \emph{least squares solution of} $x\beta=y$ --- such that
\[
    x\hat\beta = \hat y.
\]
The vector $\hat\beta$ is characterized by the fact that it minimizes
the sum of squared errors in approximating $y$ by a vector of the form $x\beta$:
\[
    \hat \beta = \argmin_{\beta \in \RR^{p\times 1}}\|x\beta - y\|^2
\]

Since $\rank(x)=p$, the matrix $x^Tx\in\RR^{p\times p}$ is invertible and the system
\[
    x^Tx\beta = x^Ty
\]
has unique solution; this solution is just $\hat\beta$:
\[
    \hat \beta = (x^Tx)^{-1}x^Ty
\]
Thus,
\[
\hat y = x\hat \beta = Py,
\]
where
\[
    P := x(x^Tx)^{-1}x^T.
\]
The matrix $P$ is called the \emph{projection matrix} because it describes
orthogonal projection from $\RR^{n\times 1}$ onto $C(x)$.

If we view the $y_i$ as realizations of random variable $Y_i$ and let
\[
    Y = \begin{bmatrix}
        Y_1\\\vdots\\Y_n,
    \end{bmatrix}
\]
then we may view 
\[
    \hat\beta = \hat\beta(Y_1,\ldots,Y_n) = (x^Tx)^{-1}x^T Y
\]
as an estimator.

\begin{theorem}
    Suppose $\rank(x)=p$ and
    \[
        Y\sim N(x\beta, \Sigma).
    \]
    Then $\hat\beta$ is an unbiased estimator of $\beta$.
\end{theorem}
\begin{proof} Use the linearity of expectation:
    \begin{multline*}
        \qquad\qquad\EE\hat \beta 
        = \EE\left[(x^Tx)^{-1}x^TY\right] 
        = (x^Tx)^{-1}x^T\EE Y
        \\= (x^Tx)^{-1}x^T (x\beta) 
        = (x^Tx)^{-1}(x^T x)\beta 
        = I\beta 
        = \beta\qquad\qquad\qedhere
    \end{multline*}        
\end{proof}

\begin{align*}
    \Var\hat\beta &= \Var (x^Tx)^{-1}x^TY\\
    &= (x^Tx)^{-1}x^T(\Var Y)((x^Tx)^{-1}x^T)^T\\
    &= (x^Tx)^{-1}x^T(\sigma^2I)x(x^Tx)^{-1}\\
    &= \sigma^2(x^Tx)^{-1}(x^Tx)(x^Tx)^{-1}\\
    &= \sigma^2(x^Tx)^{-1}
\end{align*}



\section{The case $\rank(x)\leq p$}

We consider a \emph{regularized} version of multiple linear regression.
Let $\lambda > 0$ and consider the problem of minimizing 
\[
    \SSE_\lambda(\beta) := \|x\beta - y\|^2 + \lambda^2\|\beta\|^2
\]
Let
\[
    \xi\ := \begin{bmatrix}
        x\\\lambda I^{p\times p}
    \end{bmatrix}\in\RR^{(n + p)\times p},
    \quad
    \eta := \begin{bmatrix}
        y\\0^{p\times 1}
    \end{bmatrix}\in\RR^{(n+p)\times 1}
\]
and consider the equation
\[
    \xi\beta = \eta.
\]
The columns of $\xi$ are linearly independent (why?),
so $\rank(\xi)=p$.
Therefore, by the discussion of the previous section,
$\xi^T\xi$ is invertible and 
\[
    \hat{\beta}_\lambda := (\xi^T\xi)^{-1}\xi^T\eta
\]
minimizes
\[
    \|\xi\beta - \eta\|^2 = \left\|\begin{bmatrix}
        x\beta - y\\\lambda\beta
    \end{bmatrix}\right\|^2 = \|x\beta - y\|^2 + \lambda^2\|\beta\|^2
    = \SSE_\lambda(\beta).
\]

We have:
\begin{align*}
    \xi^T\xi &= \begin{bmatrix}
        x^T&&\lambda I
    \end{bmatrix}\begin{bmatrix}
        x\\\lambda I
    \end{bmatrix}\\
    &= x^Tx + \lambda^2I,\\
    \xi^T\eta &= \begin{bmatrix}
        x^T&&\lambda I
    \end{bmatrix}
    \begin{bmatrix}
        y\\0^{p\times 1}
    \end{bmatrix}\\
    &= x^Ty
\end{align*}

Therefore,
\[
    \hat\beta_\lambda = (x^Tx + \lambda^2I)^{-1}x^Ty.
\]

Let
\[
    W_\lambda = (x^Tx + \lambda^2I)^{-1}x^Tx.
\]

\begin{theorem}
    Suppose $\rank(x)=p$, so that $\hat\beta$ is defined. Then
    \[
        \beta_\lambda = W_\lambda\hat\beta.
    \]
\end{theorem}

\begin{proof} Just compute.
    \begin{align*}
        W_\lambda\hat\beta
        &= (x^Tx + \lambda^2I)^{-1}x^Tx\hat\beta\\
        &= (x^Tx + \lambda^2I)^{-1}x^Tx(x^Tx)^{-1}x^T y\\
        &= (x^Tx + \lambda^2I)^{-1}x^T y\\
        &= \hat\beta_\lambda.\qedhere
    \end{align*}
\end{proof}

\[
    E = (a + bx - y)^2 + \lambda(a^2 + b^2)
\]

\begin{align*}
    E_x = 0 && (1 + \lambda)a + xb &= y\\
    E_y = 0 && xa + (x^2+\lambda)b &= xy
\end{align*}
Solution:
\[
    \hat a = \frac y{1+\lambda + x^2},\qquad
    \hat b = \frac {xy}{1+\lambda + x^2}
\]

\[
    \lambda = 0:\qquad \begin{bmatrix}
        \hat a\\\hat b
    \end{bmatrix} = \frac y{1+x^2}\begin{bmatrix}
        1\\x
    \end{bmatrix}\in C\left(\begin{bmatrix}1 & x\end{bmatrix}^T\right)
\]

\[
    \begin{bmatrix}
        \hat a\\\hat b
    \end{bmatrix}
    = \frac y{1+x^2}\begin{bmatrix}
        1\\x
    \end{bmatrix}
    \quad\text{is the least squares solution of}\quad
    \begin{bmatrix}1 & x\end{bmatrix}\begin{bmatrix}
        a\\b
    \end{bmatrix}
    =\begin{bmatrix}
        y
    \end{bmatrix}
\]

\section{The singular value decomposition}
\begin{theorem}[Singular Value Decomposotion, subspace formulation]\label{T:SVD}
    Let $A:\RR^n\to \RR^m$ have rank $r$.
    Then there are orthonormal bases 
    \[
        u_1,\ldots,u_r\quad\text{of}\quad C(A^T)\subseteq \RR^n
    \]
    and
    \[
        v_1,\ldots,v_r\quad\text{of}\quad C(A)\subseteq \RR^m
    \]
    and numbers
    \[
        \sigma_1\geq\sigma_2\geq\cdots\geq \sigma_r>0
    \]
    such that
    \[
        Au_i = \sigma_i v_i\quad\text{for}\quad i=1,\ldots,r.
    \]
    The $\sigma_i$ are uniquely determined by $A$.
\end{theorem}

\begin{corollary}[Singular value decomposotion, basis formulation]
    Let $A\in\RR^{m\times n}$ have rank $r$. Then there are orthogonal matrices
    \[U\in\RR^{n\times n}\quad\text{and}\quad V\in\RR^{m\times m}\]
    and numbers
    \[
        \sigma_1\geq\sigma_2\geq\cdots\geq \sigma_r>0
    \]
    such that
    \[
        A = V\,\Sigma\, U^T
    \]
    where $\Sigma\in\RR^{m\times n}$ is defined by
    \begin{equation}\label{E:Sigma}
        \Sigma_{ij} = \begin{cases}
            \sigma_i&\text{if $i=j<r$,}\\
            0&\text{otherwise.}
        \end{cases}
    \end{equation}
    The $\sigma_i$ are uniquely determined by $A$.
\end{corollary}
\begin{proof}
    By Theorem~\ref{T:SVD}, there are orthonormal bases
    \[
        u_1,\ldots,u_r\quad\text{of}\quad C(A^T)\subseteq \RR^n
    \]
    and
    \[
        v_1,\ldots,v_r\quad\text{of}\quad C(A)\subseteq \RR^m
    \]
    and numbers
    \[
        \sigma_1\geq\sigma_2\geq\cdots\geq \sigma_r>0
    \]
    such that
    \begin{equation}\label{E:plskdjf}
        Au_i = \sigma_i v_i\quad\text{for}\quad i=1,\ldots,r.
    \end{equation}
    Let
    \[
    u_{r+1},\ldots,u_{n}\quad
    \text{be an orthonormal basis of}\quad
    C(A^T)^\perp=N(A)
    \]
    and let
    \[
        v_{r+1},\ldots,u_{m}\quad
        \text{be an orthonormal basis of}\quad
        C(A)^\perp=N(A^T).
        \]
    Then $u_1,\ldots,u_r,u_{r+1},\ldots, u_n$ and
    $v_1,\ldots,v_r,v_{r+1},\ldots,v_m$ are orthonormal bases
    of $\RR^n$ and $\RR^m$, respectively.
    Equivalently,
    \[
        U:=\begin{bmatrix}
            u_1&\cdots& u_n
        \end{bmatrix}\in\RR^{n\times n}
        \quad\text{and}\quad
        V:=\begin{bmatrix}
            v_1&\cdots& v_m
        \end{bmatrix}\in\RR^{m\times m},
    \]
    are orthogonal matrices.
    Define $\Sigma\in\RR^{m\times n}$ by~\eqref{E:Sigma}.
    Since $U$ is orthogonal,
    \[
        U^Tu_i = U^{-1}u_i = e_i.
    \]
    Therefore,
    \[
    V\Sigma U^Tu_i = V\Sigma e_i = 
        V(\sigma_ie_i) = \sigma_i Ve_i = \sigma_i v_i \overset{\eqref{E:plskdjf}}= Au_i,        
    \]
    for $i=1,\ldots,r$ and
    \[
    V\Sigma U^Tu_i = V\Sigma e_i = 
        V0 = 0 = Au_i,        
    \]
    for $i=r+1,\ldots,n$. (We have $Au_i=0$ for $i=r+1,\ldots,n$
    because $u_i\in C(A^T)^\perp = N(A)$.)
    Since $u_1,\ldots,u_n$ is a basis of $\RR^n$ and $Au_i=V\Sigma U^T u_i$
    for all $i$, we must have $A = V\Sigma U^T$.
\end{proof}

\[A = U\Sigma V^T\Longleftrightarrow AV = U\sigma
\Longleftrightarrow Av_j = \sigma_j u_j\]

\begin{definition}
    The vector $v_j$ is called the \emph{$j$-th principal component} of $A$.
\end{definition}

Let $X_1,\ldots,X_p$ be $n$-dimensional random variables such that
\[
    \E{X_i}=0
\]
for all $i$. Let
\[
    X = \begin{bmatrix}
        X_1&\cdots& X_p
    \end{bmatrix}
\]
Then $$K := \E{X^TX}$$ is the covariance matrix of the random \emph{row} vector $X$.

\[
    \argmax_a\dfrac {\var(Xa)}{\|a\|^2}
    =\argmax_a\dfrac{a^T K a}{\|a\|^2}
    =\argmax_{\|a\|=1} a^T K a
\]




\section{Regularization}

Let $x_1>0$.
Suppose we want to fit a line of the form $y=\hat b x$ to the one point data set,
$\{(x_1, Y_1)\}$, where
\[
    Y_1\sim N(bx_1,\sigma^2).
\]
The parameters $b$ and $\sigma^2$ are unknown.
In this case, a natural estimator of $b$ is
\[
    \hat b = \hat b(Y_1) = \frac1{x_1}Y_1.
\]

Let $x_0>0$ and let $Y_0\sim N(bx_0, \sigma^2)$ be independent of $Y_1$.
We want to compute the expected prediction error
\[
    \EPE := \EE\left[(\hat b(Y_1) x_0 - Y_0)^2\right]
\]

\[
    \EE\left[(\hat b(Y_1) x_0 - Y_0)^2\right] = 
    x_0^2\EE\left[\hat b(Y_1)^2\right] - 2x_0\EE\left[\hat b(Y_1)Y_0\right] + \EE\left[Y_0^2\right]
\]

\begin{align*}
    \EE\left[\hat b(Y_1)^2\right]
    &= \frac1{x_1^2}\EE\left[Y_1^2\right]\\
    &= \frac1{x_1^2}\left(\EE\left[Y_1\right]^2 + \Var Y_1  \right)\\
    &= \frac1{x_1^2}\left(b^2x_1^2 + \sigma^2\right)\\
    &= b^2 + \frac{\sigma^2}{x_1^2}
\end{align*}

Since $Y_0$ and $Y_1$ are independent,
\[
    \EE\left[\hat b(Y_1)Y_0\right]
    = \EE\left[\hat b(Y_1)\right]\EE\left[Y_0\right] = b(bx_0) = b^2x_0
\]

\[
    \EE\left[Y_0^2\right] = b^2x_0^2 + \sigma^2
\]

Therefore,
\begin{align*}
    \EPE &= x_0^2\left(b^2 + \frac{\sigma^2}{x_1^2}\right)
    - 2 b^2x_0^2 + (b^2x_0^2 + \sigma^2)\\
    & = \sigma^2 + \frac{x_0^2\sigma^2}{x_1^2}
\end{align*}

\begin{align*}
    \Bias\left(\hat b(Y_1)x_0, bx_0\right)
    & = \EE\left[\hat b(Y_1)x_0\right] - bx_0\\
    & = bx_0 - bx_0\\
    & = 0
    \intertext{and}
    \Var \left(\hat b(Y_1)x_0\right)
    &= x_0^2\Var \left(\frac1{x_1}Y_1\right)\\
    &= \frac{x_0^2\sigma^2}{x_1^2}
\end{align*}

Thus, we recover the bias-variance decomposition:
\[
    \EPE = \sigma^2 
    + \Var \left(\hat b(Y_1)x_0\right)
    + \Bias\left(\hat b(Y_1)x_0, bx_0\right)^2 
\]
(Is $\hat b$ an UMVU?)

Can we find an estimator, $\tilde b(Y_1)$, of $b$, possibly biased, such that
\[
    \EE\left[(\tilde b(Y_1) x_0 - Y_0)^2\right] <
    \EE\left[(\hat b(Y_1) x_0 - Y_0)^2\right]\text{?}
\]

For $\lambda \geq 0$, define
\[
    \hat b_\lambda := \frac1{x_1 + \lambda} Y_1
\]

\[
    \E{(\hat b_\lambda(Y_1) x_0 - Y_0)^2}
    =x_0^2\E{\hat b_\lambda(Y_1)^2}
    - 2x_0\E{\hat b_\lambda(Y_1)Y_0}
    + \E{Y_0^2}
\]

\begin{align*}
    \E{\hat b_\lambda(Y_1)^2}
    &= \frac1{(x_1 + \lambda)^2}\E{Y_1^2}\\
    &= \frac1{(x_1 + \lambda)^2}\left(\E{Y_1}^2 + \Var Y_1  \right)\\
    &= \frac1{(x_1 + \lambda)^2}\left(b^2x_1^2 + \sigma^2\right)
\end{align*}

Since $Y_0$ and $Y_1$ are independent,
\begin{align*}
    \E{\hat b_\lambda(Y_1)Y_0}
    & = \E{\hat b_\lambda(Y_1)}\E{Y_0}\\[1ex]
    & = \frac{bx_1}{x_1+\lambda}bx_0\\[1ex]
    & = \frac{b^2x_0x_1}{x_1 + \lambda}
\end{align*}

\[
    \EE\left[Y_0^2\right] = b^2x_0^2 + \sigma^2
\]

Therefore,
\begin{align*}
    \E{(\hat b_\lambda(Y_1) x_0 - Y_0)^2} &= 
    \frac{b^2x_0^2x_1^2 + x_0^2\sigma^2}{(x_1 + \lambda)^2}
    - \frac{2b^2x_0^2x_1}{x_1 + \lambda} + b^2x_0^2 + \sigma^2\\
    &= \sigma^2 + \frac{x_0^2}{(x_1 + \lambda)^2}
    \big(b^2x_1^2 + \sigma^2 \\
    &\qquad- 2b^2x_1^2 - 2b^2x_1\lambda 
    + b^2x_1^2 + 2b^2x_1\lambda + b^2\lambda^2\big)\\
    &= \sigma^2 + \frac{x_0^2}{(x_1 + \lambda)^2}
    \left(\sigma^2 + b^2\lambda^2\right)\\
    &= \sigma^2 + \frac{x_0^2\sigma^2}{(x_1 + \lambda)^2}
    + \frac{b^2\lambda^2x_0^2}{(x_1 + \lambda)^2}
\end{align*}

Double check:
\begin{align*}
    \Bias\left(\hat b_\lambda(Y_1)x_0, bx_0\right)
    & = \EE\left[\hat b_\lambda(Y_1)x_0\right] - bx_0\\
    & = \frac{bx_0x_1}{x_1+\lambda} - bx_0\\
    & = -\frac{b\lambda x_0}{x_1+\lambda}
    \intertext{and}
    \Var \left(\hat b_\lambda(Y_1)x_0\right)
    &= x_0^2\Var \left(\frac1{x_1+\lambda}Y_1\right)\\
    &= \frac{x_0^2\sigma^2}{(x_1 + \lambda)^2}
\end{align*}

So:
\[
    \E{(\hat b_\lambda x_0 - Y_0)^2}
    = \sigma^2 + \Var \left(\hat b_\lambda x_0\right) +
    \Bias\left(\hat b_\lambda x_0, bx_0\right)^2
\]

\[
    \E{(\hat b_\lambda x_0 - Y_0)^2} =
    \sigma^2 + x_0^2\sigma^2\frac{1 + (\frac{b}{\sigma})^2\lambda^2}{(x_1 + \lambda)^2},\qquad
    \E{(\hat b x_0 - Y_0)^2} = \sigma^2 + x_0^2\frac{\sigma^2}{x_1^2}
\]
Let $c = b/\sigma$. Then
\begin{align*}
    \frac1{\sigma^2x_0^2}\left(\E{(\hat b_\lambda x_0 - Y_0)^2} 
    - \E{(\hat b x_0 - Y_0)^2}\right)
    &= \frac{1+c^2\lambda^2}{(x_1+\lambda)^2} - \frac1{x_1^2}\\
    &=\frac{(1+c^2\lambda^2)x_1^2 - (x_1+\lambda)^2}{x_1^2(x_1+\lambda)^2}\\
    &=\lambda(c^2\lambda x_1^2 - 2 x_1 - \lambda)\\
    &= (c^2x_1^2 - 1)\lambda\left(\lambda - \frac{2x_1}{(c^2x_1^2 - 1)}\right)
\end{align*}

Let \[F(\lambda) = \E{(\hat b_\lambda x_0 - Y_0)^2}.\]
\begin{align*}
    F'(\lambda) &= x_0\frac d{d\lambda}\frac {\sigma^2 + b^2\lambda^2}{(x_1+\lambda)^2}\\
    &=x_0\frac{2b^2\lambda(x_1+\lambda)^2 
    - (\sigma^2 + b^2\lambda^2)2(x_1 + \lambda)}
    {(x_1+\lambda)^4}\\
    &=2x_0\frac{b^2\lambda(x_1+\lambda) 
    - (\sigma^2 + b^2\lambda^2)}
    {(x_1+\lambda)^3}\\
    &= 2x_0\frac{b^2x_1\lambda - \sigma^2}
    {(x_1+\lambda)^3}\\
\end{align*}

Since $x_0, x_1, b, \sigma^2>0$,
\[
    \lambda^* := \argmin_{\lambda \geq 0} \E{(\hat b_\lambda x_0 - Y_0)^2} = \frac{\sigma^2}{b^2x_1} > 0
\]
Moreover, since
\[
    F(0) = \E{(\hat b x_0 - Y_0)^2},
\]
we deduce that
\[
    \EPE_\lambda < \EPE.
\]

\section{$k$-fold cross validation}

$D = \{(x_1,Y_1),\ldots,(x_n, Y_n)\}$, $x_i\in\RR^p$, $Y_i\sim N(x_i\beta, \sigma^2)$ independent

Let $I_1,\ldots,I_k$ be a partition of $\{1,\ldots,n\}$
into $k$ parts of about equal size.

For $i=1,\ldots,n$ let
\[
    D_i = \{(x_i, Y_i) : i\in I_i\}
\]
and let
\[
    D_{-i} = D - D_i
\]
$(x_i, Y_i)$, $i\in I_i$, from $D$:
\[
    D_i = D \setminus \{(x_i, Y_i) : i \in I_i\}
\]

For $j=1,\ldots,n$, let $\hat\theta_{-i}$
be an estimator of $\beta$ computed using the dataset $D_{-i}$.
Let
\[
    \MSE_i = \E{\frac1n\sum_{i\in I_i}(x_i\hat\theta_{-i} - Y_i)^2}
\]
be the mean square error computed using the dataset $D_i$ and set
\[
    \CV_k = \frac1k\sum_{i=1}^k\MSE_i.
\]
We use $\CV_k$ as a proxy for prediction error.
We tune any tuneable (hyper)parameters to minimize $\CV_k$.

When $k=1$, this is called \emph{leave one out cross validation (LOOCV)}.
In practice, $k$ is often $5$ or $10$.

\section{More ridge regression}

Let $t>0$. Suppose that $x^*$ is a solution of
\begin{center}
    minimize $\|x - b\|^2$ subject to $\|x\|^2=t^2$.
\end{center}
and that $x^{**}$ is a solution of
\begin{center}
    minimize $\|x - b\|^2$ subject to $\|x\|^2\leq t^2$.
\end{center}
For $r>0$, let
\[
    S_r = \{x\in\RR^n : \|x\|=r\},\quad 
    B_r = \{x\in\RR^n : \|x\|< r\}.
\]
\begin{theorem}
    Let $r>0$ and define
    \[
        x^* = \argmin_{x\in S_r}f(x),\quad
        x^{**} = \argmin_{x\in B_r\cup S_r}f(x).
    \]
    Let $\lambda^*$ be the Lagrange multiplier such that
    \[
        \nabla_{x,\lambda} f(x^*) + \lambda^*\nabla_{x,\lambda} g(x^*)=0,
    \]
    where $g(x):= \|x\|^2 - r^2$. Assume $\lambda^*\neq 0$.
    Then $x^*=x^{**}$ if and only if $\lambda^*>0$.

\end{theorem}
Let
\[
    f(x) = \|Ax-b\|^2,\quad
    g(x)=\|x\|^2,\quad\text{and}\quad
    L(x,\lambda)=f(x) + \lambda g(x).
\]
Then there is a Lagrange multiplier $\lambda^*$ such that
\[
    \nabla_{x, \lambda} L(x^*,\lambda^*) = 0.
\]

Thus,
\begin{equation}\label{E:lagrange}
    \nabla_x f(x^*) = -\lambda\nabla_xg(x^*).
\end{equation}
Now $\nabla_x f(x^*)$ points in the direction of steepest ascent of
$f(x)$ at $x=x^*$ while $\nabla_x g(x^*)$ is an outward-pointing
normal vector to the solution set of $g(x)=0$, i.e., to the sphere
of radius $t$ centered at $0$.

Suppose $\lambda^*> 0$. Since $\nabla_x g(x^*)\neq 0$ (why?),
$\nabla_xf(x^*)\neq 0$. Moreover, $\nabla_x f(x^*)$ and $\nabla_x g(x^*)$
point in the opposite directions. Since the latter is an outward-pointing
normal to the $g(x)=0$, the former points inward.
Thus, $f(x)$ increases as $x$ moves into $B(0, t)$ from $x^*$.
It follows that $x^*$



\textbf{Claim:}
Suppose $x^*\neq x^{**}$.
Then $\lambda^* < 0$.

(Equivalently, if $\lambda^*\geq 0$, then $x^* = x^{**}$.)



By the strict convexity of $f(x)$,
\begin{align*}
    f(x^* + \delta v) &= f(x^* + \delta (x^{**} - x^*))\\
    & = f((1-\delta)x^* + \delta x^{**})\\
    & < (1-\delta) f(x^*) + \delta f(x^{**})\\
    & \leq (1-\delta) f(x^*) + \delta f(x^*)\\
    & = f(x^*)
\end{align*}
As $\nabla f\cdot v$ coincides with the directional derivative in the $v$-direction,
\[
    \nabla f(x^*)\cdot v = \lim_{\delta\downarrow 0}\frac{f(x^* + \delta v) - f(x^*)}\delta < 0.
\]
It follows that $\nabla f(x^*)$ points outward from $S_r$.
But so does $\nabla g(x^*)$.
Therefore, by ~\eqref{E:lagrange}, we must have $\lambda^* < 0$.

Therefore, 
Let $v$ be the vector pointing from $x^*$ to $x^{**}$
Since $f(x^{**})\leq f(x^*)$. Since $f(x)$ is convex,
\[
    0 \geq f((t-1)x^*) - f(tx^{**}) \geq f((t-1))
\]


\begin{theorem}
    Suppose
    \begin{equation}\label{E:unconstrained}
        x^* = \argmin_x\left( \|Ax-b\| + \lambda \|x\|^2\right),
    \end{equation}
    where $\lambda>0$.
    Then
    \[
        x^*=\argmin_{\|x\|\leq \|x^*\|}\|Ax-b\|.
    \]
    Conversely, suppose
    \[
        \argmin_{\|x\|\leq r}\|Ax-b\| 
        = \argmin_{\|x\|=r}\|Ax-b\|=: x^*;
    \]
    Then there is a $\lambda^*>0$ such that
    \[
        x^* = \argmin_x\left(\|Ax-b\| + \lambda^* \|x\|^2\right).
    \]
\end{theorem}

\begin{proof}
    Suppose~\eqref{E:unconstrained} holds for some $\lambda >0$.
    Set
    \[
        f(x) = \|Ax-b\|,\quad
        g(x)=\|x\|^2 - \|x^*\|^2,
        \quad\text{and}\quad
        h(x) = f(x) + \lambda g(x).
    \]
    Then
    \[
    \nabla_x h(x) = \nabla_x \left( \|Ax-b\| + \lambda \|x\|^2\right) = 0
    \]

\end{proof}
\end{document}