\documentclass[12pt]{amsart}

\usepackage{mathrsfs, fullpage, amsmath, amssymb, graphicx, xcolor, tikz}

\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}

\input{../../macros.tex}

\begin{document}
\title{Regularization}
\maketitle

\section{Multiple linear regression}
Convention: We view $\RR^k$ as a subset of $\RR^{k+1}$ via the following identification
\begin{equation}\label{E:identification}
    v\in\RR^k\quad\longleftrightarrow\quad (1, v)\in\RR^{k+1}.
\end{equation}
    
$p-1$ predictor variables:
\[
(x_1, y_1),\ldots,(x_n, y_n)\in \RR^{1\times (p-1)}\times \RR
\]

Viewing $x_i$ as an element of $\RR^{1\times p}$ via~\eqref{E:identification}, define:
\[
    x := \begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}\in\RR^{n\times p},
    \quad y := \begin{bmatrix}
        y_1\\\vdots\\y_n
    \end{bmatrix}\in\RR^{n\times 1}
\]
For $\beta\in\RR^{p\times 1}$, consider the equation:
\[
    x\beta = y
\]
Equivalently:
\[
    \beta_0 + \beta_1x_{i,1} + \cdots \beta_{p-1}x_{i,p-1} = y_i,\quad i=1,\ldots,n.
\]

\textbf{Recall:}
The \emph{column space of $x$} is the subspace $C(x)$ of $\RR^{n\times 1}$
characterized by any of the following equivalent conditions:
\begin{itemize}\setlength\itemsep{0.5em}
    \item $C(x)$ is the set of all linear combinations of the columns of $x$
    \item $C(x)=\{x\beta : \beta\in\RR^{p\times 1}\}$
    \item $C(x)=\{y\in\RR^{n\times 1} : \text{$x\beta=y$ has a solution}\}$
\end{itemize}
$C(x)$ is also called the \emph{image of $x$}.

Let $\hat y\in\RR^{n\times 1}$ be the vector characterized by any of the
equivalent conditions:
\begin{itemize}\setlength\itemsep{0.5em}
    \item $\displaystyle{\hat y = \argmin\limits_{z\in C(x)}\|z - y\|}$
    \item $\hat y$ is the vector in the column space of $x$ closest to $y$.
    \item $\hat y$ is the orthogonal projection of $y$ onto the column space of $x$.
\end{itemize}
In partricular, $x\beta = \hat y$ has a solution.

\section{The case $\rank(x)=p$}
Suppose $\rank(x)=p$. Then $\beta\mapsto x\beta$ maps $\RR^{p\times 1}$
bijectively onto $C(x)$ and, therefore, there is a unique vector $\hat\beta\in\RR^{p\times 1}$
--- the \emph{least squares solution of} $x\beta=y$ --- such that
\[
    x\hat\beta = \hat y.
\]
The vector $\hat\beta$ is characterized by the fact that it minimizes
the sum of squared errors in approximating $y$ by a vector of the form $x\beta$:
\[
    \hat \beta = \argmin_{\beta \in \RR^{p\times 1}}\|x\beta - y\|^2
\]

Since $\rank(x)=p$, the matrix $x^Tx\in\RR^{p\times p}$ is invertible and the system
\[
    x^Tx\beta = x^Ty
\]
has unique solution; this solution is just $\hat\beta$:
\[
    \hat \beta = (x^Tx)^{-1}x^Ty
\]
Thus,
\[
\hat y = x\hat \beta = Py,
\]
where
\[
    P := x(x^Tx)^{-1}x^T.
\]
The matrix $P$ is called the \emph{projection matrix} because it describes
orthogonal projection from $\RR^{n\times 1}$ onto $C(x)$.

If we view the $y_i$ as realizations of random variable $Y_i$ and let
\[
    Y = \begin{bmatrix}
        Y_1\\\vdots\\Y_n,
    \end{bmatrix}
\]
then we may view 
\[
    \hat\beta = \hat\beta(Y_1,\ldots,Y_n) = (x^Tx)^{-1}x^T Y
\]
as an estimator.

\begin{theorem}
    Suppose $\rank(x)=p$ and
    \[
        Y\sim N(x\beta, \Sigma).
    \]
    Then $\hat\beta$ is an unbiased estimator of $\beta$.
\end{theorem}
\begin{proof} Use the linearity of expectation:
    \begin{multline*}
        \qquad\qquad\EE\hat \beta 
        = \EE\left[(x^Tx)^{-1}x^TY\right] 
        = (x^Tx)^{-1}x^T\EE Y
        \\= (x^Tx)^{-1}x^T (x\beta) 
        = (x^Tx)^{-1}(x^T x)\beta 
        = I\beta 
        = \beta\qquad\qquad\qedhere
    \end{multline*}        
\end{proof}

\begin{align*}
    \Var\hat\beta &= \Var (x^Tx)^{-1}x^TY\\
    &= (x^Tx)^{-1}x^T(\Var Y)((x^Tx)^{-1}x^T)^T\\
    &= (x^Tx)^{-1}x^T(\sigma^2I)x(x^Tx)^{-1}\\
    &= \sigma^2(x^Tx)^{-1}(x^Tx)(x^Tx)^{-1}\\
    &= \sigma^2(x^Tx)^{-1}
\end{align*}



\section{The case $\rank(x)\leq p$}

We consider a \emph{regularized} version of multiple linear regression.
Let $\lambda > 0$ and consider the problem of minimizing 
\[
    \SSE_\lambda(\beta) := \|x\beta - y\|^2 + \lambda^2\|\beta\|^2
\]
Let
\[
    \xi\ := \begin{bmatrix}
        x\\\lambda I^{p\times p}
    \end{bmatrix}\in\RR^{(n + p)\times p},
    \quad
    \eta := \begin{bmatrix}
        y\\0^{p\times 1}
    \end{bmatrix}\in\RR^{(n+p)\times 1}
\]
and consider the equation
\[
    \xi\beta = \eta.
\]
The columns of $\xi$ are linearly independent (why?),
so $\rank(\xi)=p$.
Therefore, by the discussion of the previous section,
$\xi^T\xi$ is invertible and 
\[
    \hat{\beta}_\lambda := (\xi^T\xi)^{-1}\xi^T\eta
\]
minimizes
\[
    \|\xi\beta - \eta\|^2 = \left\|\begin{bmatrix}
        x\beta - y\\\lambda\beta
    \end{bmatrix}\right\|^2 = \|x\beta - y\|^2 + \lambda^2\|\beta\|^2
    = \SSE_\lambda(\beta).
\]

We have:
\begin{align*}
    \xi^T\xi &= \begin{bmatrix}
        x^T&&\lambda I
    \end{bmatrix}\begin{bmatrix}
        x\\\lambda I
    \end{bmatrix}\\
    &= x^Tx + \lambda^2I,\\
    \xi^T\eta &= \begin{bmatrix}
        x^T&&\lambda I
    \end{bmatrix}
    \begin{bmatrix}
        y\\0^{p\times 1}
    \end{bmatrix}\\
    &= x^Ty
\end{align*}

Therefore,
\[
    \hat\beta_\lambda = (x^Tx + \lambda^2I)^{-1}x^Ty.
\]

Let
\[
    W_\lambda = (x^Tx + \lambda^2I)^{-1}x^Tx.
\]

\begin{theorem}
    Suppose $\rank(x)=p$, so that $\hat\beta$ is defined. Then
    \[
        \beta_\lambda = W_\lambda\hat\beta.
    \]
\end{theorem}

\begin{proof} Just compute.
    \begin{align*}
        W_\lambda\hat\beta
        &= (x^Tx + \lambda^2I)^{-1}x^Tx\hat\beta\\
        &= (x^Tx + \lambda^2I)^{-1}x^Tx(x^Tx)^{-1}x^T y\\
        &= (x^Tx + \lambda^2I)^{-1}x^T y\\
        &= \hat\beta_\lambda.\qedhere
    \end{align*}
\end{proof}

\[
    E = (a + bx - y)^2 + \lambda(a^2 + b^2)
\]

\begin{align*}
    E_x = 0 && (1 + \lambda)a + xb &= y\\
    E_y = 0 && xa + (x^2+\lambda)b &= xy
\end{align*}
Solution:
\[
    \hat a = \frac y{1+\lambda + x^2},\qquad
    \hat b = \frac {xy}{1+\lambda + x^2}
\]

\[
    \lambda = 0:\qquad \begin{bmatrix}
        \hat a\\\hat b
    \end{bmatrix} = \frac y{1+x^2}\begin{bmatrix}
        1\\x
    \end{bmatrix}\in C\left(\begin{bmatrix}1 & x\end{bmatrix}^T\right)
\]

\[
    \begin{bmatrix}
        \hat a\\\hat b
    \end{bmatrix}
    = \frac y{1+x^2}\begin{bmatrix}
        1\\x
    \end{bmatrix}
    \quad\text{is the least squares solution of}\quad
    \begin{bmatrix}1 & x\end{bmatrix}\begin{bmatrix}
        a\\b
    \end{bmatrix}
    =\begin{bmatrix}
        y
    \end{bmatrix}
\]

\section{The singular value decomposition}
\begin{theorem}[Singular Value Decomposotion, subspace formulation]\label{T:SVD}
    Let $A:\RR^n\to \RR^m$ have rank $r$.
    Then there are orthonormal bases 
    \[
        u_1,\ldots,u_r\quad\text{of}\quad C(A^T)\subseteq \RR^n
    \]
    and
    \[
        v_1,\ldots,v_r\quad\text{of}\quad C(A)\subseteq \RR^m
    \]
    and numbers
    \[
        \sigma_1\geq\sigma_2\geq\cdots\geq \sigma_r>0
    \]
    such that
    \[
        Au_i = \sigma_i v_i\quad\text{for}\quad i=1,\ldots,r.
    \]
    The $\sigma_i$ are uniquely determined by $A$.
\end{theorem}

\begin{corollary}[Singular value decomposotion, basis formulation]
    Let $A\in\RR^{m\times n}$ have rank $r$. Then there are orthogonal matrices
    \[U\in\RR^{n\times n}\quad\text{and}\quad V\in\RR^{m\times m}\]
    and numbers
    \[
        \sigma_1\geq\sigma_2\geq\cdots\geq \sigma_r>0
    \]
    such that
    \[
        A = V\,\Sigma\, U^T
    \]
    where $\Sigma\in\RR^{m\times n}$ is defined by
    \begin{equation}\label{E:Sigma}
        \Sigma_{ij} = \begin{cases}
            \sigma_i&\text{if $i=j<r$,}\\
            0&\text{otherwise.}
        \end{cases}
    \end{equation}
    The $\sigma_i$ are uniquely determined by $A$.
\end{corollary}
\begin{proof}
    By Theorem~\ref{T:SVD}, there are orthonormal bases
    \[
        u_1,\ldots,u_r\quad\text{of}\quad C(A^T)\subseteq \RR^n
    \]
    and
    \[
        v_1,\ldots,v_r\quad\text{of}\quad C(A)\subseteq \RR^m
    \]
    and numbers
    \[
        \sigma_1\geq\sigma_2\geq\cdots\geq \sigma_r>0
    \]
    such that
    \begin{equation}\label{E:plskdjf}
        Au_i = \sigma_i v_i\quad\text{for}\quad i=1,\ldots,r.
    \end{equation}
    Let
    \[
    u_{r+1},\ldots,u_{n}\quad
    \text{be an orthonormal basis of}\quad
    C(A^T)^\perp=N(A)
    \]
    and let
    \[
        v_{r+1},\ldots,u_{m}\quad
        \text{be an orthonormal basis of}\quad
        C(A)^\perp=N(A^T).
        \]
    Then $u_1,\ldots,u_r,u_{r+1},\ldots, u_n$ and
    $v_1,\ldots,v_r,v_{r+1},\ldots,v_m$ are orthonormal bases
    of $\RR^n$ and $\RR^m$, respectively.
    Equivalently,
    \[
        U:=\begin{bmatrix}
            u_1&\cdots& u_n
        \end{bmatrix}\in\RR^{n\times n}
        \quad\text{and}\quad
        V:=\begin{bmatrix}
            v_1&\cdots& v_m
        \end{bmatrix}\in\RR^{m\times m},
    \]
    are orthogonal matrices.
    Define $\Sigma\in\RR^{m\times n}$ by~\eqref{E:Sigma}.
    Since $U$ is orthogonal,
    \[
        U^Tu_i = U^{-1}u_i = e_i.
    \]
    Therefore,
    \[
    V\Sigma U^Tu_i = V\Sigma e_i = 
        V(\sigma_ie_i) = \sigma_i Ve_i = \sigma_i v_i \overset{\eqref{E:plskdjf}}= Au_i,        
    \]
    for $i=1,\ldots,r$ and
    \[
    V\Sigma U^Tu_i = V\Sigma e_i = 
        V0 = 0 = Au_i,        
    \]
    for $i=r+1,\ldots,n$. (We have $Au_i=0$ for $i=r+1,\ldots,n$
    because $u_i\in C(A^T)^\perp = N(A)$.)
    Since $u_1,\ldots,u_n$ is a basis of $\RR^n$ and $Au_i=V\Sigma U^T u_i$
    for all $i$, we must have $A = V\Sigma U^T$.
\end{proof}

\section{Regularization}

Let $x_1>0$.
Suppose we want to fit a line of the form $y=\hat b x$ to the one point data set,
$\{(x_1, Y_1)\}$, where
\[
    Y_1\sim N(bx_1,\sigma^2).
\]
The parameters $b$ and $\sigma^2$ are unknown.
In this case, a natural estimator of $b$ is
\[
    \hat b = \hat b(Y_1) = \frac1{x_1}Y_1.
\]

Let $x_0>0$ and let $Y_0\sim N(bx_0, \sigma^2)$ be independent of $Y_1$.
We want to compute the expected prediction error
\[
    \EPE := \EE\left[(\hat b(Y_1) x_0 - Y_0)^2\right]
\]

\[
    \EE\left[(\hat b(Y_1) x_0 - Y_0)^2\right] = 
    x_0^2\EE\left[\hat b(Y_1)^2\right] - 2x_0\EE\left[\hat b(Y_1)Y_0\right] + \EE\left[Y_0^2\right]
\]

\begin{align*}
    \EE\left[\hat b(Y_1)^2\right]
    &= \frac1{x_1^2}\EE\left[Y_1^2\right]\\
    &= \frac1{x_1^2}\left(\EE\left[Y_1\right]^2 + \Var Y_1  \right)\\
    &= \frac1{x_1^2}\left(b^2x_1^2 + \sigma^2\right)\\
    &= b^2 + \frac{\sigma^2}{x_1^2}
\end{align*}

Since $Y_0$ and $Y_1$ are independent,
\[
    \EE\left[\hat b(Y_1)Y_0\right]
    = \EE\left[\hat b(Y_1)\right]\EE\left[Y_0\right] = b(bx_0) = b^2x_0
\]

\[
    \EE\left[Y_0^2\right] = b^2x_0^2 + \sigma^2
\]

Therefore,
\begin{align*}
    \EPE &= x_0^2\left(b^2 + \frac{\sigma^2}{x_1^2}\right)
    - 2 b^2x_0^2 + (b^2x_0^2 + \sigma^2)\\
    & = \sigma^2 + \frac{x_0^2\sigma^2}{x_1^2}
\end{align*}

\begin{align*}
    \Bias\left(\hat b(Y_1)x_0, bx_0\right)
    & = \EE\left[\hat b(Y_1)x_0\right] - bx_0\\
    & = bx_0 - bx_0\\
    & = 0
    \intertext{and}
    \Var \left(\hat b(Y_1)x_0\right)
    &= x_0^2\Var \left(\frac1{x_1}Y_1\right)\\
    &= \frac{x_0^2\sigma^2}{x_1^2}
\end{align*}

Thus, we recover the bias-variance decomposition:
\[
    \EPE = \sigma^2 
    + \Var \left(\hat b(Y_1)x_0\right)
    + \Bias\left(\hat b(Y_1)x_0, bx_0\right)^2 
\]
(Is $\hat b$ an UMVU?)

Can we find an estimator, $\tilde b(Y_1)$, of $b$, possibly biased, such that
\[
    \EE\left[(\tilde b(Y_1) x_0 - Y_0)^2\right] <
    \EE\left[(\hat b(Y_1) x_0 - Y_0)^2\right]\text{?}
\]

For $\lambda \geq 0$, define
\[
    \hat b_\lambda := \frac1{x_1 + \lambda} Y_1
\]

\[
    \E{(\hat b_\lambda(Y_1) x_0 - Y_0)^2}
    =x_0^2\E{\hat b_\lambda(Y_1)^2}
    - 2x_0\E{\hat b_\lambda(Y_1)Y_0}
    + \E{Y_0^2}
\]

\begin{align*}
    \E{\hat b_\lambda(Y_1)^2}
    &= \frac1{(x_1 + \lambda)^2}\E{Y_1^2}\\
    &= \frac1{(x_1 + \lambda)^2}\left(\E{Y_1}^2 + \Var Y_1  \right)\\
    &= \frac1{(x_1 + \lambda)^2}\left(b^2x_1^2 + \sigma^2\right)
\end{align*}

Since $Y_0$ and $Y_1$ are independent,
\begin{align*}
    \E{\hat b_\lambda(Y_1)Y_0}
    & = \E{\hat b_\lambda(Y_1)}\E{Y_0}\\[1ex]
    & = \frac{bx_1}{x_1+\lambda}bx_0\\[1ex]
    & = \frac{b^2x_0x_1}{x_1 + \lambda}
\end{align*}

\[
    \EE\left[Y_0^2\right] = b^2x_0^2 + \sigma^2
\]

Therefore,
\begin{align*}
    \E{(\hat b_\lambda(Y_1) x_0 - Y_0)^2} &= 
    \frac{b^2x_0^2x_1^2 + x_0^2\sigma^2}{(x_1 + \lambda)^2}
    - \frac{2b^2x_0^2x_1}{x_1 + \lambda} + b^2x_0^2 + \sigma^2\\
    &= \sigma^2 + \frac{x_0^2}{(x_1 + \lambda)^2}
    \big(b^2x_1^2 + \sigma^2 \\
    &\qquad- 2b^2x_1^2 - 2b^2x_1\lambda 
    + b^2x_1^2 + 2b^2x_1\lambda + b^2\lambda^2\big)\\
    &= \sigma^2 + \frac{x_0^2}{(x_1 + \lambda)^2}
    \left(\sigma^2 + b^2\lambda^2\right)\\
    &= \sigma^2 + \frac{x_0^2\sigma^2}{(x_1 + \lambda)^2}
    + \frac{b^2\lambda^2x_0^2}{(x_1 + \lambda)^2}
\end{align*}

Double check:
\begin{align*}
    \Bias\left(\hat b_\lambda(Y_1)x_0, bx_0\right)
    & = \EE\left[\hat b_\lambda(Y_1)x_0\right] - bx_0\\
    & = \frac{bx_0x_1}{x_1+\lambda} - bx_0\\
    & = -\frac{b\lambda x_0}{x_1+\lambda}
    \intertext{and}
    \Var \left(\hat b_\lambda(Y_1)x_0\right)
    &= x_0^2\Var \left(\frac1{x_1+\lambda}Y_1\right)\\
    &= \frac{x_0^2\sigma^2}{(x_1 + \lambda)^2}
\end{align*}

So:
\[
    \E{(\hat b_\lambda x_0 - Y_0)^2}
    = \sigma^2 + \Var \left(\hat b_\lambda x_0\right) +
    \Bias\left(\hat b_\lambda x_0, bx_0\right)^2
\]

\[
    \E{(\hat b_\lambda x_0 - Y_0)^2} =
    \sigma^2 + x_0^2\sigma^2\frac{1 + (\frac{b}{\sigma})^2\lambda^2}{(x_1 + \lambda)^2},\qquad
    \E{(\hat b x_0 - Y_0)^2} = \sigma^2 + x_0^2\frac{\sigma^2}{x_1^2}
\]
Let $c = b/\sigma$. Then
\begin{align*}
    \frac1{\sigma^2x_0^2}\left(\E{(\hat b_\lambda x_0 - Y_0)^2} 
    - \E{(\hat b x_0 - Y_0)^2}\right)
    &= \frac{1+c^2\lambda^2}{(x_1+\lambda)^2} - \frac1{x_1^2}\\
    &=\frac{(1+c^2\lambda^2)x_1^2 - (x_1+\lambda)^2}{x_1^2(x_1+\lambda)^2}\\
    &=\lambda(c^2\lambda x_1^2 - 2 x_1 - \lambda)\\
    &= (c^2x_1^2 - 1)\lambda\left(\lambda - \frac{2x_1}{(c^2x_1^2 - 1)}\right)
\end{align*}

Let \[F(\lambda) = \E{(\hat b_\lambda x_0 - Y_0)^2}.\]
\begin{align*}
    F'(\lambda) &= x_0\frac d{d\lambda}\frac {\sigma^2 + b^2\lambda^2}{(x_1+\lambda)^2}\\
    &=x_0\frac{2b^2\lambda(x_1+\lambda)^2 
    - (\sigma^2 + b^2\lambda^2)2(x_1 + \lambda)}
    {(x_1+\lambda)^4}\\
    &=2x_0\frac{b^2\lambda(x_1+\lambda) 
    - (\sigma^2 + b^2\lambda^2)}
    {(x_1+\lambda)^3}\\
    &= 2x_0\frac{b^2x_1\lambda - \sigma^2}
    {(x_1+\lambda)^3}\\
\end{align*}

Since $x_0, x_1, b, \sigma^2>0$,
\[
    \lambda^* := \argmin_{\lambda \geq 0} \E{(\hat b_\lambda x_0 - Y_0)^2} = \frac{\sigma^2}{b^2x_1} > 0
\]
Moreover, since
\[
    F(0) = \E{(\hat b x_0 - Y_0)^2},
\]
we deduce that
\[
    \EPE_\lambda < \EPE.
\]
\end{document}