\documentclass[12pt]{amsart}

\input{../macros.tex}

\newcommand{\vmu}{\boldsymbol{\mu}}
\newcommand{\vSigma}{\boldsymbol{\Sigma}}

\begin{document}
    \title{STAT 543/641 -- Winter 2019 -- Homework \#2}

    \author{Due March ??, 2019}
    \maketitle

    \bigskip\hrule\bigskip

    Let $X_1,\ldots,X_m$ and $Y_1,\ldots,Y_n$ be random samples from populations with 
    means $\mu_X$ and $\mu_Y$ and variances $\sigma_X^2$ and $\sigma_Y^2$, respectively.
    Let $S_X^2$ and $S_Y^2$ be the standard unbiased estimators of $\sigma_X^2$ and $\sigma_Y^2$,
    respectively.
    \begin{enumerate}\setlength\itemsep{0.5em}
    \item Suppose $\sigma_X^2 = \sigma_Y^2$ and write $\sigma^2$ for this common value.
    \[
        S := \frac{(m-1)S_X^2 + (n-1)S_Y^2}{m + n - 2}
    \]
    is an unbiased estimator of $\sigma^2$. It's called the \emph{pooled variance estimator}.

    \item Suppose, in addition to having common variance,
    that the $X_i$ are independent of the $Y_i$.
    What is the distribution of $S_X^2$?
    What is its variance?
    Compare the mean squared errors $S_X^2$, $S_Y^2$, and $S^2$.

    \item Generalize these results from the case of $K=2$ populations to that of an arbitrary $K$.
    Compare with equation~(4.15) in~\cite{ISLR}.

    \item {}\textbf{[Bonus]} Can you prove analogous results with covariance matrices in place of variances?
    \end{enumerate}
    
    \bigskip\hrule\bigskip
    \cite[Exercise 12.16]{CB}
    This exercise examines an extreme case in which the likelihood equations for logistic regression have no solution.
    
    \noindent
    Consider the following $20$-point data set:
    \begin{multline*}
        (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)\\
        (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)
    \end{multline*}
    \begin{enumerate}\setlength\itemsep{0.5em}
        \item Observe that, empirically, $\Prob(Y=1|X=0)=1$ and $\Prob(Y=1|X=1)=0.5$.
    Let $\sigma(t)=(1+e^{-t})^{-1}$ be the sigmoid function.
    Are there $a$ and $b$ such that
    $\sigma(a + b\cdot 0) = 1$ and $\sigma(a+b\cdot 1)=0.5$?

    \item Let $\cL(a,b)$ be the likelihood function associated to fitting a
    logistic regression model to this data set. Show that
    \[
        L := \lim_{b\to\infty}\cL(-b, b) = \sup_{(a, b)\in\RR^2}\cL(a,b) < \infty
    \]
    and that $\cL(a,b)\neq L$ for any $(a,b)\in\RR^2$.
    What are
    \[
        \lim_{b\to\infty}\sigma(-b + b\cdot 0)\quad\text{and}\quad
        \lim_{b\to\infty}\sigma(-b + b\cdot 1)?
    \]
    \end{enumerate}


    Let $(\vX, Y)$ be jointly distributed,
    where $\vX$ is a $p$-dimensional random vector and $Y$ takes values in $\{1,\ldots,K\}$.
    Suppose that, for each $k$, $\vX|Y=k$ has Gaussian distribution with mean $\vmu_k$ and
    and variance $\Sigma$, with the latter independent of $k$.

    Consider a data set $(\vx^{(1)}, y^{(1)}),\ldots,(\vx^{(n)}, y^{(n)})$, where
    $\vx^{(i)}\in\RR^{1\times p}$ and $y^{(i)}\in\{1,\ldots,K\}$.
    For $1\leq k\leq K$, let
    \[
        I_k = \{i : y^{(i)} = k\},\quad n_k = |I_k|,\quad \hat\pi_k = \frac{n_k}{n}.
    \]
    Define sample means $\vmu_k$ and a pooled sample covariance $\vSigma$ by
    \begin{align*}
        \hat \vmu_k =\hat\vmu_{k,\vx}&= \frac1{n_k}\sum_{i\in I_k} \vx^{(i)}\in\RR^{p\times 1},\\
        \hat \vSigma =\hat\vSigma_{\vx}&= \frac1{n-K}\sum_{k=1}^K\sum_{i\in I_k}
        (\vx^{(i)} - \hat \vmu_k)^T(\vx^{(i)} - \hat \vmu_k)\in\RR^{p\times p}.
    \end{align*}
    Define linear discriminant functions, $\delta_k=\delta_{k,\vx}$, by
    \[
        \delta_k(\vv)=\delta_{k,\vx}(\vv) = \vv\hat\vSigma\hat\vmu_k^T - \frac12 \hat\vmu_k\hat\vSigma\hat\vmu_k^T + \log\hat\pi_k,
        \quad \vv\in\RR^{p\times 1}.
    \]
    
    Let $\va\in\RR^{p\times 1}$ and let
    \[
        \vw^{(i)} = \vx^{(i)} - \va.
    \]
    \[
        \hat\vmu_{k,\vw} = \hat\vmu_{k,\vx} - \va,\quad
        \Sigma_w = \Sigma_x
    \]
    \[
        \delta_{k_1,w}(v - a) - \delta_{k_2, w}(v-a) = \delta_{k_1,x}(v) - \delta_{k_2,x}(v) 
    \]
    Let $U\in\RR^{p\times p}$ be an orthogonal matrix and let $w^{(i)}=Ux^{(i)}$.
    Then
    \[
        \delta_{k, Ux}(Uv) = \delta_x(v).
    \]
    \[
        \sum_{k=1}^K\pi_k\mu_k = \mu
    \]
    \hfill

    Logistic regression (with and without ridge regularization, with and without PCA), LDA, Gaussian na\"ive Bayes, for breast cancer data set. Plot in 2d with decision boundary. Optional: Lasso

    Document classification with multinomial na\"ive Bayes

    Ridge regression via constrained optimization.



    \begin{thebibliography}{99}
        \bibitem{CB}Casella, Bergger, \emph{Statistical Inference (2nd ed.)}, Duxbury, 2002.
        % \bibitem{HMC}Hogg, McKean, Craig, \emph{Introduction to Mathematical Statistics (7th ed.)}, Pearson, 2013.
        \bibitem{ISLR}
    \end{thebibliography}


\end{document}