\documentclass[12pt]{exam}

\usepackage{amsmath, amssymb, amsthm, url}
\input{../macros.tex}
\newcommand{\sol}{\bigskip\noindent\textbf{Solution: }}

\newcommand{\vmu}{\boldsymbol{\mu}}
\newcommand{\vSigma}{\boldsymbol{\Sigma}}
\DeclareMathOperator{\Cat}{Categorical}

\begin{document}
\title{STAT 543/641 -- Winter 2019 -- Homework \#2}

\date{Due Wednesday, March 20, 2019}
\maketitle


\textbf{Notation:} Suppose $(x_1,y_1,\ldots,(x_n, y_n)\in\RR\times \{0,1\}$. Set
\begin{align*}
    \sigma(t) &= \frac1{1+e^{-t}}\\[2ex]
    p_i(a,b) &= \sigma(a + bx_i)^{y_i}\big(1 - \sigma(a + bx_i)\big)^{1-y_i}\\[2ex]
    \ell_i(a,b) &= -\log p_i(a,b)\\
    &= -y_i\log\sigma(a + bx_i) - (1-y_i)\log\big(1-\sigma(a + bx_i)\big)\\[2ex]
    \ell(a,b) &= \sum_{i=1}^n \ell_i(a,b)
\end{align*}



\begin{questions}

\question In this problem, we establish a sufficient condition for the uniqueness of
maximum likelihood estimates for univariate logistic regression coefficients,
assuming such estimates exist.

\begin{parts}\setlength\itemsep{1em}
    \part Prove: 
    \[
        \sigma'(x) = \sigma(x)\big(1-\sigma(x)\big)\tag{$*$}
    \]
    Conclude that $\sigma'(x)>0$ for all $x$.

    \part Prove that
    \[
        \nabla \ell_i(a,b) = (\sigma(a + bx_i) - y_i)\begin{bmatrix}1\\x_i\end{bmatrix}
            \qquad \text{(Hint: Use $(*)$.)}
    \]
    and that
    \[
        \nabla^2\ell_i(a,b) = \sigma'(a+bx_i)\begin{bmatrix}
            1 & x_i\\
            x_i & x_i^2
        \end{bmatrix}.
    \]
    Deduce that $\nabla^2 \ell_i(a,b)$ is positive-semidefinite but not positive-definite,
    making $\ell_i(a,b)$ convex but not strictly convex.

    \part\label{hint} Find a basis of the nullspace $N(\nabla^2\ell_i(a,b))$ of $\nabla^2\ell_i(a,b)$ whose elements do not depend on $a$ and $b$.
    
    \part Suppose that there are indices $i$ and $j$ such that $x_i\neq x_j$.
    Prove that
    \[
        \bigcap_{i=1}^n N(\nabla^2\ell_i(a,b)) = \left\{
            \begin{bmatrix}
                0\\0
            \end{bmatrix}
            \right\}.
    \]
    (Hint: Use~(\ref{hint}).)

    \item Suppose that there are indices $i$ and $j$ such that $x_i\neq x_j$.
    Show that $\nabla^2 \ell(a,b)$ is positive-definite and, hence, that $\ell(a,b)$ is strictly convex.
    
    \item Conclude that if that there are indices $i$ and $j$ such that $x_i\neq x_j$, then
    maximum likelihood estimates for $\hat a$ and $\hat b$ are unique if they exist.
\end{parts}

    
    \question In this problem, we establish a sufficient condition for the existence of
    maximum likelihood estimates for univariate logistic regression coefficients.

    Consider fitting a univariate logistic regression model to a dataset
    $(x_1, y_1),\ldots,(x_n, y_n)$ satisfying
    \[
        x_1<x_2<\cdots<x_n.
    \]


    \begin{parts}
        \part Prove that $\ell_i(a,b)>0$ for all $(a,b)\in\RR^2$.

        \part Let
        \[
            H_i = \left\{\begin{bmatrix}
                v_1\\v_2
            \end{bmatrix}\in\RR^2 : \lim_{t\to\infty}\ell_i(tv_1, tv_2) = \infty\right\}
        \]
        Find a vector $\vw\in\RR^2$ such that $H_i=H(\vw_i)$, where
        \[
            H(\vw_i) = \left\{\vv\in\RR^2 : \vv\cdot\vw_i > 0\right\}.
        \]

        \part
        Let $\vu,\vv,\vw\in\RR^2-\{\vzero\}$. Show that
        \[
            H(\vu) \cup H(\vv) \cup H(\vw) = \RR^2-\{\vzero\}
        \]
        if and only there are $a,b>0$ such that $-\vu = a\vv + v\vw$.

        \part Consider the following condition on a triple of indices $(i,j,k)$:
        \[
            \text{$y_i = y_k = 0$ and $y_j=1$}
            \qquad\text{\textit{or}}\qquad
            \text{$y_i = y_k = 1$ and $y_j=0$}\tag{\dag}
        \]
        Suppose $1\leq i<j<k\leq n$. 
        Prove that $(i,j,k)$ satisfies (\dag) if and only if
        \[
            H_i\cup H_j \cup H_k = \RR^2-\{\vzero\}.
        \]

        \part Suppose that $(i,j,k)$ is an increasing sequence of indices that satisfies (\dag).
        Prove that, for all $K>0$, the set
        \[
            S_K := \{(a,b) : \ell(a,b) \leq K\}
        \]
        contains no ray from the origin, i.e., no set of the form
        $\{t\vv : t\geq 0\}$ where $\vv\in\RR^2-\{\vzero\}$.

        \part Use the following facts to deduce that $S_K$ is bounded for all $K>0$.
            \begin{subparts}
                \subpart If $f:\RR^n\to\RR$ is a convex function,
            then $\{\vx\in\RR^n : f(\vx)| \leq K\}$ is a convex set.
            \subpart If $C$ is a convex set that contains no ray, then $C$ is bounded.
            \end{subparts} 
        
        \part Let $K>0$ be such that $S_K$ is nonempty and let
        \[
            m = \inf_{(a,b)\in S_K}\ell(a,b).
        \]
        Explain why there exists a point $(\hat a, \hat b)\in S_K$ such that $\ell(\hat a, \hat b)=m$
        and why $m$ is, in fact, the global minimum of $\ell$.

        \part Prove that $(\hat a, \hat b)$ is the unique point at which $\ell$ takes on its minimum value.


    \end{parts}

    \question
    Let $X_1,\ldots,X_m$ and $Y_1,\ldots,Y_n$ be random samples from normally distributed populations with 
means $\mu_X$ and $\mu_Y$ and variances $\sigma_X^2$ and $\sigma_Y^2$, respectively.
Let $S_X^2$ and $S_Y^2$ be the standard unbiased estimators of $\sigma_X^2$ and $\sigma_Y^2$,
respectively. Show that
\begin{parts}
\part Suppose $\sigma_X^2 = \sigma_Y^2$ and write $\sigma^2$ for this common value. Show that
\[
    S^2 := \frac{(m-1)S_X^2 + (n-1)S_Y^2}{m + n - 2}
\]
is an unbiased estimator of $\sigma^2$. It's called the \emph{pooled variance estimator}.

\part Suppose, in addition to having common variance,
that the $X_i$ are independent of the $Y_i$.
What is the distribution of
\[
\frac{(m+n-2)S^2}{\sigma^2}?
\]
What is the variance of $S^2$?



\part (Do not hand in.) Generalize these results from the case of $K=2$ populations to that of an arbitrary $K$.
Compare with equation~(4.15) in~\cite{ISLR}.

\part (Do not hand in.) Can you prove analogous results with covariance matrices in place of scalar variances?
\end{parts}

\question 
Use whatever software package you want to do this problem.
I recommend \texttt{R} or \texttt{python} with \texttt{sklearn}, though.

Load the file \texttt{dataset\_1.csv}, containing synthetic data for a binary classification problem with two numerical features.
These features are in the first two columns of the file; the target is in the third column.

\begin{parts}
\part Make a scatter plot of the data, using different colors or marker shapes to distinguish class labels.

\part Fit the data to a logistic regression model and to a Gaussian na\"ive Bayes model.
Plot the decision boundaries. Do you expect these classifiers to yield satisfactory results?
\part Write $X_1$ and $X_2$ for the two features.
Augment the dataset with three new features:
\[
    X_3:= X_1^2,\quad X_4:= X_1X_2,\quad X_5:= X_2^2
\]

\part Fit the data to a logistic regression model and to a Gaussian na\"ive Bayes model.

\part Let
\[
    c_0 + c_1X_1 + c_2X_2 + c_3X_3 + c_4X_4 + c_5X_5 = 0.
\]
be the decision boundary. Since it's graph is a hyperplane in $\RR^5$, we can't plot it.
We can, however, plot its inverse image in $\RR^2$ under the map
\[
    (x_1, x_2)\mapsto(x_1, x_2, x_1^2, x_1x_2, x_2^2),
\]
i.e., the plane curve
\[
    c_0 + c_1x_1 + c_2x_2 + c_3x_1^2 + c_4x_1x_2 + c_5x_2^2 = 0.
\]
Plot the decision boundary. Comment on your results.

\part Repeat with the dataset in \texttt{dataset\_2.csv},
augmenting your dataset with all the monomials in $X_1$ and $X_2$ of degrees $2$ and $3$, for a total
of nine features.
\end{parts}

\question
Load the file \texttt{dataset\_3.csv}, containing synthetic data for a four class classification problem with two numerical features.
These features are in the first two columns of the file; the target is in the third column.
We will use a \emph{one-versus-rest} approach to construct a four class classifier out of four binary classifiers.

\begin{parts}
\part Make a scatter plot of the data, using different colors or marker shapes to distinguish class labels.

\part Split the dataset into training and testing subsets.
Let $\vy_{tr}$ and $\vy_{te}$ be their target vectors.

\part Let $k_0\in\{0, 1, 2, 3\}$. 
Construct a new training and testing target vectors, $\vy_{tr, k}$ and $\vy_{te, k}$, vector by replacing the three class labels $k\neq k_0$ with $4$.
Fit the resulting training set to a logistic regression model.

\part Test your four models on their corresponding training sets: For each test observation, $x$,
your models will give estimates for $p(k|x)$.
Assign each test observation to the class label, $k$, for which this probability is the highest.
What is the overall relative accuracy of your classifier on the test set?

\part (Bonus) Can you plot the decision boundaries?
\end{parts}

\end{questions}


    \begin{thebibliography}{99}
        \bibitem{ISLR} Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani,
        \emph{Introduction to Statistical Learning Theory with Applications in R},
        \url{http://www-bcf.usc.edu/~gareth/ISL/}.
    \end{thebibliography}


\end{document}